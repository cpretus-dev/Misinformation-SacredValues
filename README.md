# Misinformation-SacredValues

We launched surveys in Spain (Study 1, N = 812) and the US (Study 2, N = 797), and conducted a neuroimaging study (N = 36) asking participants to rate the likelihood of sharing a series of social media posts with false statements composed by different political leaders and public figures who are part of, or openly support, the party they voted for in the last elections. 

Because conservatives and far-right supporters share more misinformation than other groups in the political spectrum (Guess et al., 2019, Garett & Bond, 2021), we chose to focus on voters of two conservative political parties in Spain (center-right and far-right) in Study 1, voters of Republican Donald J. Trump in the US in Study 2, and voters of the far-right in Spain for the neuroimaging study.

Sacred values: In the three studies, half of the posts, designed to look like tweets, included conservative sacred values (immigration, nationalism, and women and family values) and the other half non-sacred values (roads and infrastructure, foreign affairs, and waste management and materials). 

Moral-emotional language: Because previous studies suggest an effect of moral-emotional language in online content sharing (Brady et al., 2018), all items were formulated twice, once using moral-emotional language and once using neutral language (see further details in Materials). 

Fact-checking: In Study 1 and 2, participants were first asked to rate four Tweets without fact-checks (baseline block) and then rate twelve (Study 1) or eight more posts (Study 2) in a mixed block with half of the Tweets marked with a fact-check (experimental block). 

In Study 1 (N = 812) participants were split in three groups (N = 270 each) and each group was exposed to a different fact-check in the experimental block: a classic Twitter fact-check (“This claim about…is disputed.“), an accuracy-based fact-check (“To the best of your knowledge, is the above statement accurate?”, based on Pennycook et al. 2020) and a media literacy-based fact-check (“What techniques are used in this Tweet to attract your attention?”, based on content published by the Center of Media Literacy, www.medialit.org). 

In Study 2 (N = 797) half of the sample was exposed to the classic Twitter fact-check and the other half did not see any fact-checks in any of the posts.

In the neuroimaging study (N = 36), participants completed two 6-minute runs without any fact-checks (baseline block) followed by another two 6-minute runs with fact-checks in each post (experimental block). The order of these two blocks was reversed in half of the sample (N = 18).
